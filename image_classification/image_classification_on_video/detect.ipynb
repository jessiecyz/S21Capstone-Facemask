{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from math import ceil as r\n",
    "\n",
    "# imgage classification model\n",
    "from model import MobileNetV3_6c as MobileNetV3\n",
    "\n",
    "# face detection model\n",
    "from face.anchor_generator import generate_anchors\n",
    "from face.anchor_decode import decode_bbox\n",
    "from face.nms import single_class_non_max_suppression\n",
    "from face.pytorch_loader import load_pytorch_model, pytorch_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set imgage classification model\n",
    "model_path = 'model/140_6c_blur_weight.h5'\n",
    "net = MobileNetV3.build_mobilenet()\n",
    "net.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "net.build((1,64,64,3))\n",
    "net.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/.local/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'MainModel.KitModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/admin/.local/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/admin/.local/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "# set face detection model\n",
    "model = load_pytorch_model('face/model360.pth');\n",
    "feature_map_sizes = [[45, 45], [23, 23], [12, 12], [6, 6], [4, 4]]\n",
    "anchor_sizes = [[0.04, 0.056], [0.08, 0.11], [0.16, 0.22], [0.32, 0.45], [0.64, 0.72]]\n",
    "anchor_ratios = [[1, 0.62, 0.42]] * 5\n",
    "anchors = generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios)\n",
    "anchors_exp = np.expand_dims(anchors, axis=0)\n",
    "conf_thresh=0.5\n",
    "iou_thresh=0.4\n",
    "target_shape=(360, 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0:'No mask',\n",
    "    1:'Non medical full',\n",
    "    2:'Non medical partial',\n",
    "    3:'Medical full',\n",
    "    4:'Medical partial',\n",
    "    5:'Shield'\n",
    "    }\n",
    "color_dict={\n",
    "    0:(255,0,255),\n",
    "    1:(255,0,0),\n",
    "    2:(255,255,0),\n",
    "    3:(0,255,0),\n",
    "    4:(0,255,255),\n",
    "    5:(0,0,255)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_video(video_path, output_video_name):\n",
    "\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    fps = cap.get(cv.CAP_PROP_FPS)\n",
    "    w = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = 'mp4v'\n",
    "    vid_writer = cv.VideoWriter(\n",
    "        output_video_name, cv.VideoWriter_fourcc(*fourcc), fps, (w, h))\n",
    "   \n",
    "    \n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            #width, height, _ = frame.shape\n",
    "            # if the video is too big uncomment the below code\n",
    "            #frame = resize(frame, height, width)\n",
    "\n",
    "            #padding the image to avoid the bounding going out of the image\n",
    "            #and crashes the program\n",
    "            image =  cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "            #converting numpy array into image\n",
    "            #image = Image.fromarray(padding)\n",
    "            height, width, _ = image.shape\n",
    "\n",
    "            image_resized = cv.resize(image, target_shape)\n",
    "            image_np = image_resized / 255.0  # 归一化到0~1\n",
    "            image_exp = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "            image_transposed = image_exp.transpose((0, 3, 1, 2))\n",
    "\n",
    "            y_bboxes_output, y_cls_output = pytorch_inference(model, image_transposed)\n",
    "            # remove the batch dimension, for batch is always 1 for inference.\n",
    "            y_bboxes = decode_bbox(anchors_exp, y_bboxes_output)[0]\n",
    "            y_cls = y_cls_output[0]\n",
    "            # To speed up, do single class NMS, not multiple classes NMS.\n",
    "            bbox_max_scores = np.max(y_cls, axis=1)\n",
    "            bbox_max_score_classes = np.argmax(y_cls, axis=1)\n",
    "\n",
    "            # keep_idx is the alive bounding box after nms.\n",
    "            keep_idxs = single_class_non_max_suppression(y_bboxes,\n",
    "                                                         bbox_max_scores,\n",
    "                                                         conf_thresh=conf_thresh,\n",
    "                                                         iou_thresh=iou_thresh,\n",
    "                                                         )\n",
    "            people_all = 0\n",
    "            mask_detected = 0\n",
    "            \n",
    "            for idx in keep_idxs:\n",
    "                people_all += 1\n",
    "                conf = float(bbox_max_scores[idx])\n",
    "                class_id = bbox_max_score_classes[idx]\n",
    "                bbox = y_bboxes[idx]\n",
    "                # clip the coordinate, avoid the value exceed the image boundary.\n",
    "                x1 = max(0, int(bbox[0] * width))\n",
    "                y1 = max(0, int(bbox[1] * height))\n",
    "                x2 = min(int(bbox[2] * width), width)\n",
    "                y2 = min(int(bbox[3] * height), height)\n",
    "                image_test = image[y1:y2 ,x1:x2, 0:3]\n",
    "                #image_list.append(image_test)\n",
    "\n",
    "                if np.min(np.shape(image_test))<1:\n",
    "                        continue\n",
    "\n",
    "                if image.max() <= 1.0:\n",
    "                    resized = tf.image.resize_with_pad(image_test,64,64,)\n",
    "                else:\n",
    "                    resized = tf.image.resize_with_pad(image_test/255.0,64,64,)\n",
    "\n",
    "                test_images = np.zeros((1,64,64,3), dtype = float)\n",
    "\n",
    "                test_images[0] = resized.numpy()\n",
    "                pred_labels = net.predict(test_images)\n",
    "                pred = np.argmax(pred_labels, axis=1)\n",
    "                \n",
    "                if pred[0]==1 or pred[0]==3 or pred[0]==5:\n",
    "                    mask_detected += 1\n",
    "\n",
    "                scale = round((y2-y1)*35/100)\n",
    "\n",
    "                cv.rectangle(frame, (x1,y1), (x2,y2),color_dict[pred[0]],2)\n",
    "                cv.putText(frame,labels[pred[0]], \n",
    "                            (x1,y1-5),cv.FONT_HERSHEY_SIMPLEX,\n",
    "                                                    2.0,color_dict[pred[0]],2)\n",
    "            if people_all:\n",
    "                    cv.putText(frame, \"Compliance rate = %.2f %%\" % (mask_detected/people_all*100), \n",
    "                    (5, round(height/20)), cv.FONT_HERSHEY_PLAIN, 2, [255, 255, 255], 2)\n",
    "\n",
    "            vid_writer.write(frame)\n",
    "            if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('End')\n",
    "            break\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/.local/lib/python3.7/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n"
     ]
    }
   ],
   "source": [
    "run_on_video('MixedMask.mp4', 'MixedMask_6c.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
